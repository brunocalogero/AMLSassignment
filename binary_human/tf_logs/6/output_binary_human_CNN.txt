[0 0 1 ... 0 0 1]
full dataset of shape: (4546, 128, 128, 3)
full labels of shape: (4546,)
X_data of shape: (4546, 128, 128, 3)
Y_data of shape: (4546,)
X_train of shape: (2908, 128, 128, 3)
y_train of shape: (2908, 2)
X_val of shape: (728, 128, 128, 3)
y_val of shape: (728, 2)
X_test of shape: (910, 128, 128, 3)
y_test of shape: (910, 2)
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
[187.0849381  190.80914718 194.27063274 188.85350757 192.59731774
 196.07255846 189.66093535 193.36176066 196.85591472 190.05570839]
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
X_train of shape: (2908, 128, 128, 3)
X_val of shape: (728, 128, 128, 3)
X_test of shape: (910, 128, 128, 3)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 128, 32)      128       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 64, 64, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 64, 16)        4624      
_________________________________________________________________
batch_normalization_2 (Batch (None, 64, 64, 16)        64        
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 32, 16)        0         
_________________________________________________________________
dense_1 (Dense)              (None, 32, 32, 1024)      17408     
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32, 1024)      0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1048576)           0         
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 2097154   
=================================================================
Total params: 2,120,274
Trainable params: 2,120,178
Non-trainable params: 96
_________________________________________________________________
Start learning with best params at 2019-01-05 21:01:43.559928
[0 0 1 ... 0 0 1]
full dataset of shape: (4546, 128, 128, 3)
full labels of shape: (4546,)
X_data of shape: (4546, 128, 128, 3)
Y_data of shape: (4546,)
X_train of shape: (2908, 128, 128, 3)
y_train of shape: (2908, 2)
X_val of shape: (728, 128, 128, 3)
y_val of shape: (728, 2)
X_test of shape: (910, 128, 128, 3)
y_test of shape: (910, 2)
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
[187.0849381  190.80914718 194.27063274 188.85350757 192.59731774
 196.07255846 189.66093535 193.36176066 196.85591472 190.05570839]
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
X_train of shape: (2908, 128, 128, 3)
X_val of shape: (728, 128, 128, 3)
X_test of shape: (910, 128, 128, 3)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 128, 32)      128       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 64, 64, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 64, 16)        4624      
_________________________________________________________________
batch_normalization_2 (Batch (None, 64, 64, 16)        64        
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 32, 16)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32, 16)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 16384)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 32770     
=================================================================
Total params: 38,482
Trainable params: 38,386
Non-trainable params: 96
_________________________________________________________________
Start learning with best params at 2019-01-05 21:03:55.036283
Train on 2908 samples, validate on 728 samples
Epoch 1/5

  64/2908 [..............................] - ETA: 2:27 - loss: 1.0246 - acc: 0.5781
 128/2908 [>.............................] - ETA: 2:19 - loss: 0.7313 - acc: 0.7266
 192/2908 [>.............................] - ETA: 2:14 - loss: 0.6145 - acc: 0.8073
 256/2908 [=>............................] - ETA: 2:12 - loss: 0.6161 - acc: 0.8320
 320/2908 [==>...........................] - ETA: 2:08 - loss: 0.4962 - acc: 0.8625
 384/2908 [==>...........................] - ETA: 2:04 - loss: 0.5218 - acc: 0.8750
 448/2908 [===>..........................] - ETA: 2:00 - loss: 0.5355 - acc: 0.8862
 512/2908 [====>.........................] - ETA: 1:57 - loss: 0.4685 - acc: 0.9004
 576/2908 [====>.........................] - ETA: 1:53 - loss: 0.4451 - acc: 0.9097
 640/2908 [=====>........................] - ETA: 1:50 - loss: 0.4483 - acc: 0.9141
 704/2908 [======>.......................] - ETA: 1:47 - loss: 0.4283 - acc: 0.9190
 768/2908 [======>.......................] - ETA: 1:43 - loss: 0.3949 - acc: 0.9245
 832/2908 [=======>......................] - ETA: 1:40 - loss: 0.3672 - acc: 0.9291
 896/2908 [========>.....................] - ETA: 1:37 - loss: 0.3448 - acc: 0.9330
 960/2908 [========>.....................] - ETA: 1:34 - loss: 0.3498 - acc: 0.9354
1024/2908 [=========>....................] - ETA: 1:31 - loss: 0.3291 - acc: 0.9385
1088/2908 [==========>...................] - ETA: 1:27 - loss: 0.3271 - acc: 0.9403
1152/2908 [==========>...................] - ETA: 1:24 - loss: 0.3091 - acc: 0.9436
1216/2908 [===========>..................] - ETA: 1:21 - loss: 0.2958 - acc: 0.9457
1280/2908 [============>.................] - ETA: 1:18 - loss: 0.2810 - acc: 0.9484
1344/2908 [============>.................] - ETA: 1:15 - loss: 0.2742 - acc: 0.9494
1408/2908 [=============>................] - ETA: 1:12 - loss: 0.2618 - acc: 0.9517
1472/2908 [==============>...............] - ETA: 1:09 - loss: 0.2557 - acc: 0.9531
1536/2908 [==============>...............] - ETA: 1:06 - loss: 0.2524 - acc: 0.9544
1600/2908 [===============>..............] - ETA: 1:03 - loss: 0.2457 - acc: 0.9556
1664/2908 [================>.............] - ETA: 1:00 - loss: 0.2374 - acc: 0.9567
1728/2908 [================>.............] - ETA: 57s - loss: 0.2289 - acc: 0.9583 
1792/2908 [=================>............] - ETA: 53s - loss: 0.2207 - acc: 0.9598
1856/2908 [==================>...........] - ETA: 50s - loss: 0.2131 - acc: 0.9612
1920/2908 [==================>...........] - ETA: 47s - loss: 0.2060 - acc: 0.9625
1984/2908 [===================>..........] - ETA: 44s - loss: 0.1994 - acc: 0.9637
2048/2908 [====================>.........] - ETA: 41s - loss: 0.1932 - acc: 0.9648
2112/2908 [====================>.........] - ETA: 38s - loss: 0.1873 - acc: 0.9659
2176/2908 [=====================>........] - ETA: 35s - loss: 0.1819 - acc: 0.9669
2240/2908 [======================>.......] - ETA: 32s - loss: 0.1767 - acc: 0.9679
2304/2908 [======================>.......] - ETA: 29s - loss: 0.1722 - acc: 0.9683
2368/2908 [=======================>......] - ETA: 26s - loss: 0.1675 - acc: 0.9692
2432/2908 [========================>.....] - ETA: 22s - loss: 0.1631 - acc: 0.9700
2496/2908 [========================>.....] - ETA: 19s - loss: 0.1589 - acc: 0.9708
2560/2908 [=========================>....] - ETA: 16s - loss: 0.1550 - acc: 0.9715
2624/2908 [==========================>...] - ETA: 13s - loss: 0.1525 - acc: 0.9718
2688/2908 [==========================>...] - ETA: 10s - loss: 0.1489 - acc: 0.9725
2752/2908 [===========================>..] - ETA: 7s - loss: 0.1454 - acc: 0.9731 
2816/2908 [============================>.] - ETA: 4s - loss: 0.1421 - acc: 0.9737
2880/2908 [============================>.] - ETA: 1s - loss: 0.1389 - acc: 0.9743
2908/2908 [==============================] - 148s 51ms/step - loss: 0.1376 - acc: 0.9746 - val_loss: 1.7849e-05 - val_acc: 1.0000
Epoch 2/5

  64/2908 [..............................] - ETA: 2:18 - loss: 2.1234e-07 - acc: 1.0000
 128/2908 [>.............................] - ETA: 2:14 - loss: 1.7090e-07 - acc: 1.0000
 192/2908 [>.............................] - ETA: 2:12 - loss: 1.5367e-07 - acc: 1.0000
 256/2908 [=>............................] - ETA: 2:09 - loss: 1.5157e-07 - acc: 1.0000
 320/2908 [==>...........................] - ETA: 2:08 - loss: 1.4566e-07 - acc: 1.0000
 384/2908 [==>...........................] - ETA: 2:05 - loss: 2.4198e-06 - acc: 1.0000
 448/2908 [===>..........................] - ETA: 2:02 - loss: 0.0029 - acc: 0.9978    
 512/2908 [====>.........................] - ETA: 1:58 - loss: 0.0025 - acc: 0.9980
 576/2908 [====>.........................] - ETA: 1:54 - loss: 0.0023 - acc: 0.9983
 640/2908 [=====>........................] - ETA: 1:50 - loss: 0.0020 - acc: 0.9984
 704/2908 [======>.......................] - ETA: 1:47 - loss: 0.0019 - acc: 0.9986
 768/2908 [======>.......................] - ETA: 1:44 - loss: 0.0024 - acc: 0.9987
 832/2908 [=======>......................] - ETA: 1:41 - loss: 0.0022 - acc: 0.9988
 896/2908 [========>.....................] - ETA: 1:38 - loss: 0.0020 - acc: 0.9989
 960/2908 [========>.....................] - ETA: 1:35 - loss: 0.0019 - acc: 0.9990
1024/2908 [=========>....................] - ETA: 1:32 - loss: 0.0018 - acc: 0.9990
1088/2908 [==========>...................] - ETA: 1:29 - loss: 0.0017 - acc: 0.9991
1152/2908 [==========>...................] - ETA: 1:25 - loss: 0.0016 - acc: 0.9991
1216/2908 [===========>..................] - ETA: 1:22 - loss: 0.0015 - acc: 0.9992
1280/2908 [============>.................] - ETA: 1:19 - loss: 0.0014 - acc: 0.9992
1344/2908 [============>.................] - ETA: 1:16 - loss: 0.0014 - acc: 0.9993
1408/2908 [=============>................] - ETA: 1:12 - loss: 0.0013 - acc: 0.9993
1472/2908 [==============>...............] - ETA: 1:09 - loss: 0.0012 - acc: 0.9993
1536/2908 [==============>...............] - ETA: 1:06 - loss: 0.0012 - acc: 0.9993
1600/2908 [===============>..............] - ETA: 1:03 - loss: 0.0018 - acc: 0.9988
1664/2908 [================>.............] - ETA: 1:00 - loss: 0.0017 - acc: 0.9988
1728/2908 [================>.............] - ETA: 57s - loss: 0.0017 - acc: 0.9988 
1792/2908 [=================>............] - ETA: 53s - loss: 0.0016 - acc: 0.9989
1856/2908 [==================>...........] - ETA: 50s - loss: 0.0016 - acc: 0.9989
1920/2908 [==================>...........] - ETA: 47s - loss: 0.0015 - acc: 0.9990
1984/2908 [===================>..........] - ETA: 44s - loss: 0.0015 - acc: 0.9990
2048/2908 [====================>.........] - ETA: 41s - loss: 0.0014 - acc: 0.9990
2112/2908 [====================>.........] - ETA: 38s - loss: 0.0015 - acc: 0.9991
2176/2908 [=====================>........] - ETA: 35s - loss: 0.0014 - acc: 0.9991
2240/2908 [======================>.......] - ETA: 32s - loss: 0.0014 - acc: 0.9991
2304/2908 [======================>.......] - ETA: 29s - loss: 0.0014 - acc: 0.9991
2368/2908 [=======================>......] - ETA: 26s - loss: 0.0013 - acc: 0.9992
2432/2908 [========================>.....] - ETA: 22s - loss: 0.0013 - acc: 0.9992
2496/2908 [========================>.....] - ETA: 19s - loss: 0.0076 - acc: 0.9988
2560/2908 [=========================>....] - ETA: 16s - loss: 0.0074 - acc: 0.9988
2624/2908 [==========================>...] - ETA: 13s - loss: 0.0072 - acc: 0.9989
2688/2908 [==========================>...] - ETA: 10s - loss: 0.0071 - acc: 0.9989
2752/2908 [===========================>..] - ETA: 7s - loss: 0.0069 - acc: 0.9989 
2816/2908 [============================>.] - ETA: 4s - loss: 0.0067 - acc: 0.9989
2880/2908 [============================>.] - ETA: 1s - loss: 0.0066 - acc: 0.9990
2908/2908 [==============================] - 149s 51ms/step - loss: 0.0065 - acc: 0.9990 - val_loss: 1.2868e-04 - val_acc: 1.0000
Epoch 3/5

  64/2908 [..............................] - ETA: 2:14 - loss: 2.0142e-05 - acc: 1.0000
 128/2908 [>.............................] - ETA: 2:11 - loss: 1.0131e-05 - acc: 1.0000
 192/2908 [>.............................] - ETA: 2:10 - loss: 6.7936e-06 - acc: 1.0000
 256/2908 [=>............................] - ETA: 2:07 - loss: 4.0961e-05 - acc: 1.0000
 320/2908 [==>...........................] - ETA: 2:05 - loss: 0.0417 - acc: 0.9969    
 384/2908 [==>...........................] - ETA: 2:03 - loss: 0.0348 - acc: 0.9974
 448/2908 [===>..........................] - ETA: 2:00 - loss: 0.0298 - acc: 0.9978
 512/2908 [====>.........................] - ETA: 1:57 - loss: 0.0399 - acc: 0.9941
 576/2908 [====>.........................] - ETA: 1:54 - loss: 0.0355 - acc: 0.9948
 640/2908 [=====>........................] - ETA: 1:51 - loss: 0.0320 - acc: 0.9953
 704/2908 [======>.......................] - ETA: 1:47 - loss: 0.0291 - acc: 0.9957
 768/2908 [======>.......................] - ETA: 1:44 - loss: 0.0267 - acc: 0.9961
 832/2908 [=======>......................] - ETA: 1:41 - loss: 0.0246 - acc: 0.9964
 896/2908 [========>.....................] - ETA: 1:38 - loss: 0.0255 - acc: 0.9955
 960/2908 [========>.....................] - ETA: 1:35 - loss: 0.0450 - acc: 0.9938
1024/2908 [=========>....................] - ETA: 1:31 - loss: 0.0445 - acc: 0.9922
1088/2908 [==========>...................] - ETA: 1:28 - loss: 0.0419 - acc: 0.9926
1152/2908 [==========>...................] - ETA: 1:25 - loss: 0.0396 - acc: 0.9931
1216/2908 [===========>..................] - ETA: 1:22 - loss: 0.0375 - acc: 0.9934
1280/2908 [============>.................] - ETA: 1:19 - loss: 0.0356 - acc: 0.9938
1344/2908 [============>.................] - ETA: 1:15 - loss: 0.0339 - acc: 0.9940
1408/2908 [=============>................] - ETA: 1:12 - loss: 0.0324 - acc: 0.9943
1472/2908 [==============>...............] - ETA: 1:09 - loss: 0.0310 - acc: 0.9946
1536/2908 [==============>...............] - ETA: 1:06 - loss: 0.0297 - acc: 0.9948
1600/2908 [===============>..............] - ETA: 1:03 - loss: 0.0286 - acc: 0.9950
1664/2908 [================>.............] - ETA: 1:00 - loss: 0.0275 - acc: 0.9952
1728/2908 [================>.............] - ETA: 57s - loss: 0.0265 - acc: 0.9954 
1792/2908 [=================>............] - ETA: 54s - loss: 0.0256 - acc: 0.9955
1856/2908 [==================>...........] - ETA: 50s - loss: 0.0247 - acc: 0.9957
1920/2908 [==================>...........] - ETA: 47s - loss: 0.0310 - acc: 0.9953
1984/2908 [===================>..........] - ETA: 44s - loss: 0.0360 - acc: 0.9950
2048/2908 [====================>.........] - ETA: 41s - loss: 0.0460 - acc: 0.9941
2112/2908 [====================>.........] - ETA: 38s - loss: 0.0446 - acc: 0.9943
2176/2908 [=====================>........] - ETA: 35s - loss: 0.0438 - acc: 0.9940
2240/2908 [======================>.......] - ETA: 32s - loss: 0.0426 - acc: 0.9942
2304/2908 [======================>.......] - ETA: 29s - loss: 0.0414 - acc: 0.9944
2368/2908 [=======================>......] - ETA: 26s - loss: 0.0403 - acc: 0.9945
2432/2908 [========================>.....] - ETA: 22s - loss: 0.0392 - acc: 0.9947
2496/2908 [========================>.....] - ETA: 19s - loss: 0.0383 - acc: 0.9948
2560/2908 [=========================>....] - ETA: 16s - loss: 0.0409 - acc: 0.9945
2624/2908 [==========================>...] - ETA: 13s - loss: 0.0431 - acc: 0.9939
2688/2908 [==========================>...] - ETA: 10s - loss: 0.0420 - acc: 0.9940
2752/2908 [===========================>..] - ETA: 7s - loss: 0.0411 - acc: 0.9942 
2816/2908 [============================>.] - ETA: 4s - loss: 0.0406 - acc: 0.9940
2880/2908 [============================>.] - ETA: 1s - loss: 0.0399 - acc: 0.9941
2908/2908 [==============================] - 148s 51ms/step - loss: 0.0395 - acc: 0.9942 - val_loss: 6.4960e-07 - val_acc: 1.0000
Epoch 4/5

  64/2908 [..............................] - ETA: 2:14 - loss: 1.1921e-07 - acc: 1.0000
 128/2908 [>.............................] - ETA: 2:13 - loss: 1.6484e-07 - acc: 1.0000
 192/2908 [>.............................] - ETA: 2:10 - loss: 1.4963e-07 - acc: 1.0000
 256/2908 [=>............................] - ETA: 2:08 - loss: 1.4203e-07 - acc: 1.0000
 320/2908 [==>...........................] - ETA: 2:05 - loss: 6.6408e-07 - acc: 1.0000
 384/2908 [==>...........................] - ETA: 2:02 - loss: 5.7808e-07 - acc: 1.0000
 448/2908 [===>..........................] - ETA: 1:59 - loss: 0.0167 - acc: 0.9978    
 512/2908 [====>.........................] - ETA: 1:57 - loss: 0.0186 - acc: 0.9961
 576/2908 [====>.........................] - ETA: 1:54 - loss: 0.0165 - acc: 0.9965
 640/2908 [=====>........................] - ETA: 1:50 - loss: 0.0149 - acc: 0.9969
 704/2908 [======>.......................] - ETA: 1:47 - loss: 0.0135 - acc: 0.9972
 768/2908 [======>.......................] - ETA: 1:44 - loss: 0.0124 - acc: 0.9974
 832/2908 [=======>......................] - ETA: 1:41 - loss: 0.0114 - acc: 0.9976
 896/2908 [========>.....................] - ETA: 1:38 - loss: 0.0111 - acc: 0.9978
 960/2908 [========>.....................] - ETA: 1:35 - loss: 0.0103 - acc: 0.9979
1024/2908 [=========>....................] - ETA: 1:32 - loss: 0.0112 - acc: 0.9971
1088/2908 [==========>...................] - ETA: 1:29 - loss: 0.0105 - acc: 0.9972
1152/2908 [==========>...................] - ETA: 1:26 - loss: 0.0099 - acc: 0.9974
1216/2908 [===========>..................] - ETA: 1:22 - loss: 0.0094 - acc: 0.9975
1280/2908 [============>.................] - ETA: 1:19 - loss: 0.0089 - acc: 0.9977
1344/2908 [============>.................] - ETA: 1:16 - loss: 0.0085 - acc: 0.9978
1408/2908 [=============>................] - ETA: 1:13 - loss: 0.0081 - acc: 0.9979
1472/2908 [==============>...............] - ETA: 1:09 - loss: 0.0078 - acc: 0.9980
1536/2908 [==============>...............] - ETA: 1:06 - loss: 0.0075 - acc: 0.9980
1600/2908 [===============>..............] - ETA: 1:03 - loss: 0.0072 - acc: 0.9981
1664/2908 [================>.............] - ETA: 1:00 - loss: 0.0069 - acc: 0.9982
1728/2908 [================>.............] - ETA: 57s - loss: 0.0066 - acc: 0.9983 
1792/2908 [=================>............] - ETA: 54s - loss: 0.0064 - acc: 0.9983
1856/2908 [==================>...........] - ETA: 51s - loss: 0.0062 - acc: 0.9984
1920/2908 [==================>...........] - ETA: 47s - loss: 0.0060 - acc: 0.9984
1984/2908 [===================>..........] - ETA: 44s - loss: 0.0058 - acc: 0.9985
2048/2908 [====================>.........] - ETA: 41s - loss: 0.0056 - acc: 0.9985
2112/2908 [====================>.........] - ETA: 38s - loss: 0.0054 - acc: 0.9986
2176/2908 [=====================>........] - ETA: 35s - loss: 0.0053 - acc: 0.9986
2240/2908 [======================>.......] - ETA: 32s - loss: 0.0051 - acc: 0.9987
2304/2908 [======================>.......] - ETA: 29s - loss: 0.0050 - acc: 0.9987
2368/2908 [=======================>......] - ETA: 26s - loss: 0.0048 - acc: 0.9987
2432/2908 [========================>.....] - ETA: 23s - loss: 0.0047 - acc: 0.9988
2496/2908 [========================>.....] - ETA: 19s - loss: 0.0046 - acc: 0.9988
2560/2908 [=========================>....] - ETA: 16s - loss: 0.0045 - acc: 0.9988
2624/2908 [==========================>...] - ETA: 13s - loss: 0.0044 - acc: 0.9989
2688/2908 [==========================>...] - ETA: 10s - loss: 0.0043 - acc: 0.9989
2752/2908 [===========================>..] - ETA: 7s - loss: 0.0042 - acc: 0.9989 
2816/2908 [============================>.] - ETA: 4s - loss: 0.0041 - acc: 0.9989
2880/2908 [============================>.] - ETA: 1s - loss: 0.0040 - acc: 0.9990
2908/2908 [==============================] - 149s 51ms/step - loss: 0.0039 - acc: 0.9990 - val_loss: 1.2698e-05 - val_acc: 1.0000
Epoch 5/5

  64/2908 [..............................] - ETA: 2:19 - loss: 1.1921e-07 - acc: 1.0000
 128/2908 [>.............................] - ETA: 2:14 - loss: 1.2526e-07 - acc: 1.0000
 192/2908 [>.............................] - ETA: 2:09 - loss: 3.0575e-05 - acc: 1.0000
 256/2908 [=>............................] - ETA: 2:06 - loss: 2.2961e-05 - acc: 1.0000
 320/2908 [==>...........................] - ETA: 2:02 - loss: 1.8393e-05 - acc: 1.0000
 384/2908 [==>...........................] - ETA: 1:59 - loss: 1.5347e-05 - acc: 1.0000
 448/2908 [===>..........................] - ETA: 1:56 - loss: 1.3172e-05 - acc: 1.0000
 512/2908 [====>.........................] - ETA: 1:53 - loss: 1.1540e-05 - acc: 1.0000
 576/2908 [====>.........................] - ETA: 1:50 - loss: 1.0272e-05 - acc: 1.0000
 640/2908 [=====>........................] - ETA: 1:47 - loss: 9.2572e-06 - acc: 1.0000
 704/2908 [======>.......................] - ETA: 1:44 - loss: 8.4268e-06 - acc: 1.0000
 768/2908 [======>.......................] - ETA: 1:41 - loss: 7.7362e-06 - acc: 1.0000
 832/2908 [=======>......................] - ETA: 1:38 - loss: 7.6360e-06 - acc: 1.0000
 896/2908 [========>.....................] - ETA: 1:35 - loss: 7.0991e-06 - acc: 1.0000
 960/2908 [========>.....................] - ETA: 1:32 - loss: 6.6449e-06 - acc: 1.0000
1024/2908 [=========>....................] - ETA: 1:29 - loss: 6.2466e-06 - acc: 1.0000
1088/2908 [==========>...................] - ETA: 1:26 - loss: 5.8862e-06 - acc: 1.0000
1152/2908 [==========>...................] - ETA: 1:23 - loss: 5.5658e-06 - acc: 1.0000
1216/2908 [===========>..................] - ETA: 1:20 - loss: 5.2791e-06 - acc: 1.0000
1280/2908 [============>.................] - ETA: 1:17 - loss: 5.0211e-06 - acc: 1.0000
1344/2908 [============>.................] - ETA: 1:14 - loss: 4.8172e-06 - acc: 1.0000
1408/2908 [=============>................] - ETA: 1:11 - loss: 4.7205e-06 - acc: 1.0000
1472/2908 [==============>...............] - ETA: 1:08 - loss: 4.5204e-06 - acc: 1.0000
1536/2908 [==============>...............] - ETA: 1:05 - loss: 4.3371e-06 - acc: 1.0000
1600/2908 [===============>..............] - ETA: 1:02 - loss: 4.1683e-06 - acc: 1.0000
1664/2908 [================>.............] - ETA: 59s - loss: 4.0126e-06 - acc: 1.0000 
1728/2908 [================>.............] - ETA: 56s - loss: 3.8684e-06 - acc: 1.0000
1792/2908 [=================>............] - ETA: 53s - loss: 3.7345e-06 - acc: 1.0000
1856/2908 [==================>...........] - ETA: 50s - loss: 3.6098e-06 - acc: 1.0000
1920/2908 [==================>...........] - ETA: 47s - loss: 3.4935e-06 - acc: 1.0000
1984/2908 [===================>..........] - ETA: 44s - loss: 3.3846e-06 - acc: 1.0000
2048/2908 [====================>.........] - ETA: 41s - loss: 3.2826e-06 - acc: 1.0000
2112/2908 [====================>.........] - ETA: 38s - loss: 3.1919e-06 - acc: 1.0000
2176/2908 [=====================>........] - ETA: 35s - loss: 3.1015e-06 - acc: 1.0000
2240/2908 [======================>.......] - ETA: 32s - loss: 3.0915e-06 - acc: 1.0000
2304/2908 [======================>.......] - ETA: 29s - loss: 3.0089e-06 - acc: 1.0000
2368/2908 [=======================>......] - ETA: 26s - loss: 2.9367e-06 - acc: 1.0000
2432/2908 [========================>.....] - ETA: 23s - loss: 2.8625e-06 - acc: 1.0000
2496/2908 [========================>.....] - ETA: 19s - loss: 2.7922e-06 - acc: 1.0000
2560/2908 [=========================>....] - ETA: 16s - loss: 2.7254e-06 - acc: 1.0000
2624/2908 [==========================>...] - ETA: 13s - loss: 2.6618e-06 - acc: 1.0000
2688/2908 [==========================>...] - ETA: 10s - loss: 2.6013e-06 - acc: 1.0000
2752/2908 [===========================>..] - ETA: 7s - loss: 2.5436e-06 - acc: 1.0000 
2816/2908 [============================>.] - ETA: 4s - loss: 2.4885e-06 - acc: 1.0000
2880/2908 [============================>.] - ETA: 1s - loss: 2.4359e-06 - acc: 1.0000
2908/2908 [==============================] - 149s 51ms/step - loss: 2.4136e-06 - acc: 1.0000 - val_loss: 7.8376e-06 - val_acc: 1.0000
Stop learning 2019-01-05 21:16:21.091361
Elapsed learning time 0:12:26.055078
[[1.0000000e+00 0.0000000e+00]
 [1.4203088e-18 1.0000000e+00]
 [1.1036660e-21 1.0000000e+00]
 ...
 [1.0000000e+00 2.3970275e-30]
 [7.3013218e-24 1.0000000e+00]
 [8.2172243e-21 1.0000000e+00]]
[[1. 0.]
 [0. 1.]
 [0. 1.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]
[0 0 1 ... 0 0 1]
full dataset of shape: (4546, 128, 128, 3)
full labels of shape: (4546,)
X_data of shape: (4546, 128, 128, 3)
Y_data of shape: (4546,)
X_train of shape: (2908, 128, 128, 3)
y_train of shape: (2908, 2)
X_val of shape: (728, 128, 128, 3)
y_val of shape: (728, 2)
X_test of shape: (910, 128, 128, 3)
y_test of shape: (910, 2)
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
[187.0849381  190.80914718 194.27063274 188.85350757 192.59731774
 196.07255846 189.66093535 193.36176066 196.85591472 190.05570839]
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
X_train of shape: (2908, 128, 128, 3)
X_val of shape: (728, 128, 128, 3)
X_test of shape: (910, 128, 128, 3)
[0 0 1 ... 0 0 1]
full dataset of shape: (4546, 128, 128, 3)
full labels of shape: (4546,)
X_data of shape: (4546, 128, 128, 3)
Y_data of shape: (4546,)
X_train of shape: (2908, 128, 128, 3)
y_train of shape: (2908, 2)
X_val of shape: (728, 128, 128, 3)
y_val of shape: (728, 2)
X_test of shape: (910, 128, 128, 3)
y_test of shape: (910, 2)
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
[187.0849381  190.80914718 194.27063274 188.85350757 192.59731774
 196.07255846 189.66093535 193.36176066 196.85591472 190.05570839]
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
X_train of shape: (2908, 128, 128, 3)
X_val of shape: (728, 128, 128, 3)
X_test of shape: (910, 128, 128, 3)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 128, 32)      128       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 64, 64, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 64, 16)        4624      
_________________________________________________________________
batch_normalization_2 (Batch (None, 64, 64, 16)        64        
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 32, 16)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32, 16)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 16384)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 32770     
=================================================================
Total params: 38,482
Trainable params: 38,386
Non-trainable params: 96
_________________________________________________________________
Start learning with best params at 2019-01-05 22:15:37.063051
Train on 2908 samples, validate on 728 samples
Epoch 1/5

  64/2908 [..............................] - ETA: 2:23 - loss: 1.1590 - mean_absolute_error: 0.4422 - categorical_accuracy: 0.5469
 128/2908 [>.............................] - ETA: 2:15 - loss: 0.9469 - mean_absolute_error: 0.3100 - categorical_accuracy: 0.6875
 192/2908 [>.............................] - ETA: 2:11 - loss: 0.8391 - mean_absolute_error: 0.2276 - categorical_accuracy: 0.7708
 256/2908 [=>............................] - ETA: 2:08 - loss: 0.7748 - mean_absolute_error: 0.1893 - categorical_accuracy: 0.8125
 320/2908 [==>...........................] - ETA: 2:04 - loss: 0.6904 - mean_absolute_error: 0.1609 - categorical_accuracy: 0.8406
 384/2908 [==>...........................] - ETA: 2:01 - loss: 0.6225 - mean_absolute_error: 0.1416 - categorical_accuracy: 0.8594
 448/2908 [===>..........................] - ETA: 1:58 - loss: 0.5361 - mean_absolute_error: 0.1235 - categorical_accuracy: 0.8795
 512/2908 [====>.........................] - ETA: 1:55 - loss: 0.4930 - mean_absolute_error: 0.1165 - categorical_accuracy: 0.8867
 576/2908 [====>.........................] - ETA: 1:52 - loss: 0.4390 - mean_absolute_error: 0.1042 - categorical_accuracy: 0.8993
 640/2908 [=====>........................] - ETA: 1:48 - loss: 0.4049 - mean_absolute_error: 0.0953 - categorical_accuracy: 0.9078
 704/2908 [======>.......................] - ETA: 1:45 - loss: 0.3686 - mean_absolute_error: 0.0871 - categorical_accuracy: 0.9162
 768/2908 [======>.......................] - ETA: 1:42 - loss: 0.3381 - mean_absolute_error: 0.0800 - categorical_accuracy: 0.9232
 832/2908 [=======>......................] - ETA: 1:39 - loss: 0.3121 - mean_absolute_error: 0.0739 - categorical_accuracy: 0.9291
 896/2908 [========>.....................] - ETA: 1:36 - loss: 0.3027 - mean_absolute_error: 0.0732 - categorical_accuracy: 0.9297
 960/2908 [========>.....................] - ETA: 1:33 - loss: 0.2825 - mean_absolute_error: 0.0684 - categorical_accuracy: 0.9344
1024/2908 [=========>....................] - ETA: 1:30 - loss: 0.2660 - mean_absolute_error: 0.0650 - categorical_accuracy: 0.9375
1088/2908 [==========>...................] - ETA: 1:27 - loss: 0.2565 - mean_absolute_error: 0.0621 - categorical_accuracy: 0.9403
1152/2908 [==========>...................] - ETA: 1:24 - loss: 0.2465 - mean_absolute_error: 0.0600 - categorical_accuracy: 0.9418
1216/2908 [===========>..................] - ETA: 1:21 - loss: 0.2408 - mean_absolute_error: 0.0578 - categorical_accuracy: 0.9441
1280/2908 [============>.................] - ETA: 1:18 - loss: 0.2287 - mean_absolute_error: 0.0549 - categorical_accuracy: 0.9469
1344/2908 [============>.................] - ETA: 1:14 - loss: 0.2183 - mean_absolute_error: 0.0526 - categorical_accuracy: 0.9494
1408/2908 [=============>................] - ETA: 1:11 - loss: 0.2136 - mean_absolute_error: 0.0509 - categorical_accuracy: 0.9510
1472/2908 [==============>...............] - ETA: 1:08 - loss: 0.2048 - mean_absolute_error: 0.0491 - categorical_accuracy: 0.9524
1536/2908 [==============>...............] - ETA: 1:05 - loss: 0.1963 - mean_absolute_error: 0.0470 - categorical_accuracy: 0.9544
1600/2908 [===============>..............] - ETA: 1:03 - loss: 0.1884 - mean_absolute_error: 0.0451 - categorical_accuracy: 0.9563
1664/2908 [================>.............] - ETA: 59s - loss: 0.1812 - mean_absolute_error: 0.0434 - categorical_accuracy: 0.9579 
1728/2908 [================>.............] - ETA: 56s - loss: 0.1745 - mean_absolute_error: 0.0418 - categorical_accuracy: 0.9595
1792/2908 [=================>............] - ETA: 53s - loss: 0.1704 - mean_absolute_error: 0.0409 - categorical_accuracy: 0.9604
1856/2908 [==================>...........] - ETA: 50s - loss: 0.1645 - mean_absolute_error: 0.0394 - categorical_accuracy: 0.9617
1920/2908 [==================>...........] - ETA: 47s - loss: 0.1590 - mean_absolute_error: 0.0381 - categorical_accuracy: 0.9630
1984/2908 [===================>..........] - ETA: 44s - loss: 0.1539 - mean_absolute_error: 0.0369 - categorical_accuracy: 0.9642
2048/2908 [====================>.........] - ETA: 41s - loss: 0.1491 - mean_absolute_error: 0.0357 - categorical_accuracy: 0.9653
2112/2908 [====================>.........] - ETA: 38s - loss: 0.1446 - mean_absolute_error: 0.0347 - categorical_accuracy: 0.9664
2176/2908 [=====================>........] - ETA: 35s - loss: 0.1403 - mean_absolute_error: 0.0336 - categorical_accuracy: 0.9674
2240/2908 [======================>.......] - ETA: 32s - loss: 0.1365 - mean_absolute_error: 0.0328 - categorical_accuracy: 0.9683
2304/2908 [======================>.......] - ETA: 29s - loss: 0.1327 - mean_absolute_error: 0.0319 - categorical_accuracy: 0.9692
2368/2908 [=======================>......] - ETA: 26s - loss: 0.1291 - mean_absolute_error: 0.0311 - categorical_accuracy: 0.9700
2432/2908 [========================>.....] - ETA: 23s - loss: 0.1257 - mean_absolute_error: 0.0302 - categorical_accuracy: 0.9708
2496/2908 [========================>.....] - ETA: 20s - loss: 0.1225 - mean_absolute_error: 0.0295 - categorical_accuracy: 0.9716
2560/2908 [=========================>....] - ETA: 16s - loss: 0.1194 - mean_absolute_error: 0.0287 - categorical_accuracy: 0.9723
2624/2908 [==========================>...] - ETA: 13s - loss: 0.1165 - mean_absolute_error: 0.0280 - categorical_accuracy: 0.9729
2688/2908 [==========================>...] - ETA: 10s - loss: 0.1137 - mean_absolute_error: 0.0274 - categorical_accuracy: 0.9736
2752/2908 [===========================>..] - ETA: 7s - loss: 0.1111 - mean_absolute_error: 0.0267 - categorical_accuracy: 0.9742 
2816/2908 [============================>.] - ETA: 4s - loss: 0.1086 - mean_absolute_error: 0.0261 - categorical_accuracy: 0.9748
2880/2908 [============================>.] - ETA: 1s - loss: 0.1062 - mean_absolute_error: 0.0255 - categorical_accuracy: 0.9753
2908/2908 [==============================] - 150s 52ms/step - loss: 0.1051 - mean_absolute_error: 0.0253 - categorical_accuracy: 0.9756 - val_loss: 8.8610e-05 - val_mean_absolute_error: 8.7557e-05 - val_categorical_accuracy: 1.0000
Epoch 2/5

  64/2908 [..............................] - ETA: 2:28 - loss: 1.7414e-04 - mean_absolute_error: 1.7355e-04 - categorical_accuracy: 1.0000
 128/2908 [>.............................] - ETA: 2:19 - loss: 0.0018 - mean_absolute_error: 0.0016 - categorical_accuracy: 1.0000        
 192/2908 [>.............................] - ETA: 2:15 - loss: 0.0012 - mean_absolute_error: 0.0011 - categorical_accuracy: 1.0000
 256/2908 [=>............................] - ETA: 2:11 - loss: 8.9314e-04 - mean_absolute_error: 8.0900e-04 - categorical_accuracy: 1.0000
 320/2908 [==>...........................] - ETA: 2:08 - loss: 7.7196e-04 - mean_absolute_error: 7.0411e-04 - categorical_accuracy: 1.0000
 384/2908 [==>...........................] - ETA: 2:03 - loss: 6.4467e-04 - mean_absolute_error: 5.8810e-04 - categorical_accuracy: 1.0000
 448/2908 [===>..........................] - ETA: 2:00 - loss: 5.5438e-04 - mean_absolute_error: 5.0588e-04 - categorical_accuracy: 1.0000
 512/2908 [====>.........................] - ETA: 1:57 - loss: 4.8511e-04 - mean_absolute_error: 4.4266e-04 - categorical_accuracy: 1.0000
 576/2908 [====>.........................] - ETA: 1:55 - loss: 4.3123e-04 - mean_absolute_error: 3.9348e-04 - categorical_accuracy: 1.0000
 640/2908 [=====>........................] - ETA: 1:51 - loss: 3.8871e-04 - mean_absolute_error: 3.5473e-04 - categorical_accuracy: 1.0000
 704/2908 [======>.......................] - ETA: 1:48 - loss: 3.5340e-04 - mean_absolute_error: 3.2249e-04 - categorical_accuracy: 1.0000
 768/2908 [======>.......................] - ETA: 1:45 - loss: 3.2396e-04 - mean_absolute_error: 2.9562e-04 - categorical_accuracy: 1.0000
 832/2908 [=======>......................] - ETA: 1:42 - loss: 2.9905e-04 - mean_absolute_error: 2.7288e-04 - categorical_accuracy: 1.0000
 896/2908 [========>.....................] - ETA: 1:39 - loss: 2.7791e-04 - mean_absolute_error: 2.5361e-04 - categorical_accuracy: 1.0000
 960/2908 [========>.....................] - ETA: 1:36 - loss: 2.6144e-04 - mean_absolute_error: 2.3875e-04 - categorical_accuracy: 1.0000
1024/2908 [=========>....................] - ETA: 1:33 - loss: 2.4511e-04 - mean_absolute_error: 2.2383e-04 - categorical_accuracy: 1.0000
1088/2908 [==========>...................] - ETA: 1:29 - loss: 2.3072e-04 - mean_absolute_error: 2.1068e-04 - categorical_accuracy: 1.0000
1152/2908 [==========>...................] - ETA: 1:26 - loss: 2.1791e-04 - mean_absolute_error: 1.9897e-04 - categorical_accuracy: 1.0000
1216/2908 [===========>..................] - ETA: 1:23 - loss: 2.0644e-04 - mean_absolute_error: 1.8850e-04 - categorical_accuracy: 1.0000
1280/2908 [============>.................] - ETA: 1:20 - loss: 0.0015 - mean_absolute_error: 8.1973e-04 - categorical_accuracy: 0.9992    
1344/2908 [============>.................] - ETA: 1:16 - loss: 0.0015 - mean_absolute_error: 7.8070e-04 - categorical_accuracy: 0.9993
1408/2908 [=============>................] - ETA: 1:13 - loss: 0.0014 - mean_absolute_error: 7.4533e-04 - categorical_accuracy: 0.9993
1472/2908 [==============>...............] - ETA: 1:10 - loss: 0.0013 - mean_absolute_error: 7.1298e-04 - categorical_accuracy: 0.9993
1536/2908 [==============>...............] - ETA: 1:07 - loss: 0.0013 - mean_absolute_error: 6.8327e-04 - categorical_accuracy: 0.9993
1600/2908 [===============>..............] - ETA: 1:03 - loss: 0.0012 - mean_absolute_error: 6.6728e-04 - categorical_accuracy: 0.9994
1664/2908 [================>.............] - ETA: 1:00 - loss: 0.0012 - mean_absolute_error: 6.4162e-04 - categorical_accuracy: 0.9994
1728/2908 [================>.............] - ETA: 57s - loss: 0.0011 - mean_absolute_error: 6.1787e-04 - categorical_accuracy: 0.9994 
1792/2908 [=================>............] - ETA: 54s - loss: 0.0011 - mean_absolute_error: 5.9581e-04 - categorical_accuracy: 0.9994
1856/2908 [==================>...........] - ETA: 51s - loss: 0.0011 - mean_absolute_error: 5.7527e-04 - categorical_accuracy: 0.9995
1920/2908 [==================>...........] - ETA: 48s - loss: 0.0010 - mean_absolute_error: 5.5657e-04 - categorical_accuracy: 0.9995
1984/2908 [===================>..........] - ETA: 45s - loss: 9.9764e-04 - mean_absolute_error: 5.3861e-04 - categorical_accuracy: 0.9995
2048/2908 [====================>.........] - ETA: 42s - loss: 9.6646e-04 - mean_absolute_error: 5.2178e-04 - categorical_accuracy: 0.9995
2112/2908 [====================>.........] - ETA: 38s - loss: 9.3718e-04 - mean_absolute_error: 5.0597e-04 - categorical_accuracy: 0.9995
2176/2908 [=====================>........] - ETA: 35s - loss: 9.0962e-04 - mean_absolute_error: 4.9109e-04 - categorical_accuracy: 0.9995
2240/2908 [======================>.......] - ETA: 32s - loss: 8.8368e-04 - mean_absolute_error: 4.7710e-04 - categorical_accuracy: 0.9996
2304/2908 [======================>.......] - ETA: 29s - loss: 8.7414e-04 - mean_absolute_error: 4.7860e-04 - categorical_accuracy: 0.9996
2368/2908 [=======================>......] - ETA: 26s - loss: 8.5054e-04 - mean_absolute_error: 4.6569e-04 - categorical_accuracy: 0.9996
2432/2908 [========================>.....] - ETA: 23s - loss: 8.2823e-04 - mean_absolute_error: 4.5350e-04 - categorical_accuracy: 0.9996
2496/2908 [========================>.....] - ETA: 20s - loss: 8.0700e-04 - mean_absolute_error: 4.4187e-04 - categorical_accuracy: 0.9996
2560/2908 [=========================>....] - ETA: 17s - loss: 7.8802e-04 - mean_absolute_error: 4.3201e-04 - categorical_accuracy: 0.9996
2624/2908 [==========================>...] - ETA: 13s - loss: 7.6942e-04 - mean_absolute_error: 4.2210e-04 - categorical_accuracy: 0.9996
2688/2908 [==========================>...] - ETA: 10s - loss: 7.5111e-04 - mean_absolute_error: 4.1205e-04 - categorical_accuracy: 0.9996
2752/2908 [===========================>..] - ETA: 7s - loss: 7.3364e-04 - mean_absolute_error: 4.0247e-04 - categorical_accuracy: 0.9996 
2816/2908 [============================>.] - ETA: 4s - loss: 7.1702e-04 - mean_absolute_error: 3.9337e-04 - categorical_accuracy: 0.9996
2880/2908 [============================>.] - ETA: 1s - loss: 7.0108e-04 - mean_absolute_error: 3.8463e-04 - categorical_accuracy: 0.9997
2908/2908 [==============================] - 151s 52ms/step - loss: 6.9434e-04 - mean_absolute_error: 3.8092e-04 - categorical_accuracy: 0.9997 - val_loss: 1.5137e-06 - val_mean_absolute_error: 1.3985e-06 - val_categorical_accuracy: 1.0000
Epoch 3/5

  64/2908 [..............................] - ETA: 2:15 - loss: 9.6654e-06 - mean_absolute_error: 9.5462e-06 - categorical_accuracy: 1.0000
 128/2908 [>.............................] - ETA: 2:13 - loss: 1.0851e-05 - mean_absolute_error: 1.0731e-05 - categorical_accuracy: 1.0000
 192/2908 [>.............................] - ETA: 2:11 - loss: 7.2761e-06 - mean_absolute_error: 7.1582e-06 - categorical_accuracy: 1.0000
 256/2908 [=>............................] - ETA: 2:07 - loss: 5.5668e-06 - mean_absolute_error: 5.4497e-06 - categorical_accuracy: 1.0000
 320/2908 [==>...........................] - ETA: 2:05 - loss: 5.0713e-06 - mean_absolute_error: 4.9542e-06 - categorical_accuracy: 1.0000
 384/2908 [==>...........................] - ETA: 2:02 - loss: 4.2879e-06 - mean_absolute_error: 4.1707e-06 - categorical_accuracy: 1.0000
 448/2908 [===>..........................] - ETA: 1:58 - loss: 3.7126e-06 - mean_absolute_error: 3.5957e-06 - categorical_accuracy: 1.0000
 512/2908 [====>.........................] - ETA: 1:55 - loss: 1.3030e-05 - mean_absolute_error: 1.2889e-05 - categorical_accuracy: 1.0000
 576/2908 [====>.........................] - ETA: 1:52 - loss: 1.3625e-05 - mean_absolute_error: 1.3486e-05 - categorical_accuracy: 1.0000
 640/2908 [=====>........................] - ETA: 1:49 - loss: 1.2331e-05 - mean_absolute_error: 1.2194e-05 - categorical_accuracy: 1.0000
 704/2908 [======>.......................] - ETA: 1:46 - loss: 1.1221e-05 - mean_absolute_error: 1.1086e-05 - categorical_accuracy: 1.0000
 768/2908 [======>.......................] - ETA: 1:42 - loss: 1.0296e-05 - mean_absolute_error: 1.0162e-05 - categorical_accuracy: 1.0000
 832/2908 [=======>......................] - ETA: 1:39 - loss: 9.5266e-06 - mean_absolute_error: 9.3948e-06 - categorical_accuracy: 1.0000
 896/2908 [========>.....................] - ETA: 1:37 - loss: 8.8548e-06 - mean_absolute_error: 8.7241e-06 - categorical_accuracy: 1.0000
 960/2908 [========>.....................] - ETA: 1:34 - loss: 8.4381e-06 - mean_absolute_error: 8.3085e-06 - categorical_accuracy: 1.0000
1024/2908 [=========>....................] - ETA: 1:31 - loss: 7.9182e-06 - mean_absolute_error: 7.7892e-06 - categorical_accuracy: 1.0000
1088/2908 [==========>...................] - ETA: 1:28 - loss: 7.4840e-06 - mean_absolute_error: 7.3558e-06 - categorical_accuracy: 1.0000
1152/2908 [==========>...................] - ETA: 1:25 - loss: 7.0769e-06 - mean_absolute_error: 6.9496e-06 - categorical_accuracy: 1.0000
1216/2908 [===========>..................] - ETA: 1:22 - loss: 6.7107e-06 - mean_absolute_error: 6.5838e-06 - categorical_accuracy: 1.0000
1280/2908 [============>.................] - ETA: 1:19 - loss: 6.3811e-06 - mean_absolute_error: 6.2546e-06 - categorical_accuracy: 1.0000
1344/2908 [============>.................] - ETA: 1:16 - loss: 6.0871e-06 - mean_absolute_error: 5.9611e-06 - categorical_accuracy: 1.0000
1408/2908 [=============>................] - ETA: 1:13 - loss: 5.8158e-06 - mean_absolute_error: 5.6901e-06 - categorical_accuracy: 1.0000
1472/2908 [==============>...............] - ETA: 1:09 - loss: 5.5681e-06 - mean_absolute_error: 5.4427e-06 - categorical_accuracy: 1.0000
1536/2908 [==============>...............] - ETA: 1:06 - loss: 5.3447e-06 - mean_absolute_error: 5.2197e-06 - categorical_accuracy: 1.0000
1600/2908 [===============>..............] - ETA: 1:03 - loss: 4.4346e-05 - mean_absolute_error: 4.3025e-05 - categorical_accuracy: 1.0000
1664/2908 [================>.............] - ETA: 1:00 - loss: 4.3584e-05 - mean_absolute_error: 4.2308e-05 - categorical_accuracy: 1.0000
1728/2908 [================>.............] - ETA: 57s - loss: 4.1974e-05 - mean_absolute_error: 4.0741e-05 - categorical_accuracy: 1.0000 
1792/2908 [=================>............] - ETA: 54s - loss: 4.0667e-05 - mean_absolute_error: 3.9474e-05 - categorical_accuracy: 1.0000
1856/2908 [==================>...........] - ETA: 51s - loss: 3.9885e-05 - mean_absolute_error: 3.8729e-05 - categorical_accuracy: 1.0000
1920/2908 [==================>...........] - ETA: 48s - loss: 3.8610e-05 - mean_absolute_error: 3.7489e-05 - categorical_accuracy: 1.0000
1984/2908 [===================>..........] - ETA: 45s - loss: 3.8604e-05 - mean_absolute_error: 3.7515e-05 - categorical_accuracy: 1.0000
2048/2908 [====================>.........] - ETA: 42s - loss: 3.7430e-05 - mean_absolute_error: 3.6371e-05 - categorical_accuracy: 1.0000
2112/2908 [====================>.........] - ETA: 38s - loss: 3.6300e-05 - mean_absolute_error: 3.5269e-05 - categorical_accuracy: 1.0000
2176/2908 [=====================>........] - ETA: 35s - loss: 3.5235e-05 - mean_absolute_error: 3.4232e-05 - categorical_accuracy: 1.0000
2240/2908 [======================>.......] - ETA: 32s - loss: 3.4232e-05 - mean_absolute_error: 3.3254e-05 - categorical_accuracy: 1.0000
2304/2908 [======================>.......] - ETA: 29s - loss: 3.3285e-05 - mean_absolute_error: 3.2330e-05 - categorical_accuracy: 1.0000
2368/2908 [=======================>......] - ETA: 26s - loss: 4.0439e-05 - mean_absolute_error: 3.9431e-05 - categorical_accuracy: 1.0000
2432/2908 [========================>.....] - ETA: 23s - loss: 3.9378e-05 - mean_absolute_error: 3.8393e-05 - categorical_accuracy: 1.0000
2496/2908 [========================>.....] - ETA: 20s - loss: 3.9864e-05 - mean_absolute_error: 3.8899e-05 - categorical_accuracy: 1.0000
2560/2908 [=========================>....] - ETA: 16s - loss: 3.8871e-05 - mean_absolute_error: 3.7928e-05 - categorical_accuracy: 1.0000
2624/2908 [==========================>...] - ETA: 13s - loss: 3.7927e-05 - mean_absolute_error: 3.7003e-05 - categorical_accuracy: 1.0000
2688/2908 [==========================>...] - ETA: 10s - loss: 3.7027e-05 - mean_absolute_error: 3.6123e-05 - categorical_accuracy: 1.0000
2752/2908 [===========================>..] - ETA: 7s - loss: 3.6169e-05 - mean_absolute_error: 3.5283e-05 - categorical_accuracy: 1.0000 
2816/2908 [============================>.] - ETA: 4s - loss: 3.5509e-05 - mean_absolute_error: 3.4641e-05 - categorical_accuracy: 1.0000
2880/2908 [============================>.] - ETA: 1s - loss: 3.4723e-05 - mean_absolute_error: 3.3871e-05 - categorical_accuracy: 1.0000
2908/2908 [==============================] - 150s 52ms/step - loss: 3.4390e-05 - mean_absolute_error: 3.3545e-05 - categorical_accuracy: 1.0000 - val_loss: 1.3119e-06 - val_mean_absolute_error: 1.1971e-06 - val_categorical_accuracy: 1.0000
Epoch 4/5

  64/2908 [..............................] - ETA: 2:15 - loss: 1.1921e-07 - mean_absolute_error: 2.0167e-10 - categorical_accuracy: 1.0000
 128/2908 [>.............................] - ETA: 2:16 - loss: 1.1921e-07 - mean_absolute_error: 1.0346e-10 - categorical_accuracy: 1.0000
 192/2908 [>.............................] - ETA: 2:15 - loss: 3.9598e-06 - mean_absolute_error: 3.8407e-06 - categorical_accuracy: 1.0000
 256/2908 [=>............................] - ETA: 2:14 - loss: 3.0109e-06 - mean_absolute_error: 2.8922e-06 - categorical_accuracy: 1.0000
 320/2908 [==>...........................] - ETA: 2:13 - loss: 2.4325e-06 - mean_absolute_error: 2.3138e-06 - categorical_accuracy: 1.0000
 384/2908 [==>...........................] - ETA: 2:13 - loss: 5.9402e-06 - mean_absolute_error: 5.8192e-06 - categorical_accuracy: 1.0000
 448/2908 [===>..........................] - ETA: 2:08 - loss: 5.1086e-06 - mean_absolute_error: 4.9879e-06 - categorical_accuracy: 1.0000
 512/2908 [====>.........................] - ETA: 2:04 - loss: 4.4849e-06 - mean_absolute_error: 4.3645e-06 - categorical_accuracy: 1.0000
 576/2908 [====>.........................] - ETA: 2:01 - loss: 1.0096e-05 - mean_absolute_error: 9.9702e-06 - categorical_accuracy: 1.0000
 640/2908 [=====>........................] - ETA: 1:57 - loss: 1.1142e-05 - mean_absolute_error: 1.1017e-05 - categorical_accuracy: 1.0000
 704/2908 [======>.......................] - ETA: 1:53 - loss: 1.0140e-05 - mean_absolute_error: 1.0015e-05 - categorical_accuracy: 1.0000
 768/2908 [======>.......................] - ETA: 1:50 - loss: 9.4226e-06 - mean_absolute_error: 9.2990e-06 - categorical_accuracy: 1.0000
 832/2908 [=======>......................] - ETA: 1:46 - loss: 8.8967e-06 - mean_absolute_error: 8.7738e-06 - categorical_accuracy: 1.0000
 896/2908 [========>.....................] - ETA: 1:43 - loss: 8.2711e-06 - mean_absolute_error: 8.1486e-06 - categorical_accuracy: 1.0000
 960/2908 [========>.....................] - ETA: 1:39 - loss: 7.7276e-06 - mean_absolute_error: 7.6054e-06 - categorical_accuracy: 1.0000
1024/2908 [=========>....................] - ETA: 1:36 - loss: 7.2521e-06 - mean_absolute_error: 7.1300e-06 - categorical_accuracy: 1.0000
1088/2908 [==========>...................] - ETA: 1:32 - loss: 6.8325e-06 - mean_absolute_error: 6.7106e-06 - categorical_accuracy: 1.0000
1152/2908 [==========>...................] - ETA: 1:29 - loss: 6.4666e-06 - mean_absolute_error: 6.3452e-06 - categorical_accuracy: 1.0000
1216/2908 [===========>..................] - ETA: 1:25 - loss: 6.1433e-06 - mean_absolute_error: 6.0226e-06 - categorical_accuracy: 1.0000
1280/2908 [============>.................] - ETA: 1:22 - loss: 5.8644e-06 - mean_absolute_error: 5.7444e-06 - categorical_accuracy: 1.0000
1344/2908 [============>.................] - ETA: 1:19 - loss: 5.8312e-06 - mean_absolute_error: 5.7117e-06 - categorical_accuracy: 1.0000
1408/2908 [=============>................] - ETA: 1:16 - loss: 4.3513e-05 - mean_absolute_error: 4.2509e-05 - categorical_accuracy: 1.0000
1472/2908 [==============>...............] - ETA: 1:13 - loss: 4.1626e-05 - mean_absolute_error: 4.0661e-05 - categorical_accuracy: 1.0000
1536/2908 [==============>...............] - ETA: 1:09 - loss: 3.9897e-05 - mean_absolute_error: 3.8967e-05 - categorical_accuracy: 1.0000
1600/2908 [===============>..............] - ETA: 1:06 - loss: 3.8308e-05 - mean_absolute_error: 3.7410e-05 - categorical_accuracy: 1.0000
1664/2908 [================>.............] - ETA: 1:03 - loss: 3.6843e-05 - mean_absolute_error: 3.5975e-05 - categorical_accuracy: 1.0000
1728/2908 [================>.............] - ETA: 1:00 - loss: 3.5483e-05 - mean_absolute_error: 3.4643e-05 - categorical_accuracy: 1.0000
1792/2908 [=================>............] - ETA: 56s - loss: 3.4223e-05 - mean_absolute_error: 3.3409e-05 - categorical_accuracy: 1.0000 
1856/2908 [==================>...........] - ETA: 53s - loss: 3.3078e-05 - mean_absolute_error: 3.2288e-05 - categorical_accuracy: 1.0000
1920/2908 [==================>...........] - ETA: 50s - loss: 3.1984e-05 - mean_absolute_error: 3.1216e-05 - categorical_accuracy: 1.0000
1984/2908 [===================>..........] - ETA: 47s - loss: 3.0970e-05 - mean_absolute_error: 3.0224e-05 - categorical_accuracy: 1.0000
2048/2908 [====================>.........] - ETA: 43s - loss: 3.0330e-05 - mean_absolute_error: 2.9603e-05 - categorical_accuracy: 1.0000
2112/2908 [====================>.........] - ETA: 40s - loss: 2.9493e-05 - mean_absolute_error: 2.8785e-05 - categorical_accuracy: 1.0000
2176/2908 [=====================>........] - ETA: 37s - loss: 2.8640e-05 - mean_absolute_error: 2.7949e-05 - categorical_accuracy: 1.0000
2240/2908 [======================>.......] - ETA: 34s - loss: 2.7825e-05 - mean_absolute_error: 2.7151e-05 - categorical_accuracy: 1.0000
2304/2908 [======================>.......] - ETA: 30s - loss: 2.7055e-05 - mean_absolute_error: 2.6397e-05 - categorical_accuracy: 1.0000
2368/2908 [=======================>......] - ETA: 27s - loss: 2.6331e-05 - mean_absolute_error: 2.5687e-05 - categorical_accuracy: 1.0000
2432/2908 [========================>.....] - ETA: 24s - loss: 2.5641e-05 - mean_absolute_error: 2.5011e-05 - categorical_accuracy: 1.0000
2496/2908 [========================>.....] - ETA: 20s - loss: 2.4987e-05 - mean_absolute_error: 2.4370e-05 - categorical_accuracy: 1.0000
2560/2908 [=========================>....] - ETA: 17s - loss: 2.4367e-05 - mean_absolute_error: 2.3763e-05 - categorical_accuracy: 1.0000
2624/2908 [==========================>...] - ETA: 14s - loss: 2.3776e-05 - mean_absolute_error: 2.3183e-05 - categorical_accuracy: 1.0000
2688/2908 [==========================>...] - ETA: 11s - loss: 2.3221e-05 - mean_absolute_error: 2.2639e-05 - categorical_accuracy: 1.0000
2752/2908 [===========================>..] - ETA: 7s - loss: 2.2687e-05 - mean_absolute_error: 2.2116e-05 - categorical_accuracy: 1.0000 
2816/2908 [============================>.] - ETA: 4s - loss: 2.2175e-05 - mean_absolute_error: 2.1615e-05 - categorical_accuracy: 1.0000
2880/2908 [============================>.] - ETA: 1s - loss: 2.1692e-05 - mean_absolute_error: 2.1141e-05 - categorical_accuracy: 1.0000
2908/2908 [==============================] - 155s 53ms/step - loss: 2.1485e-05 - mean_absolute_error: 2.0938e-05 - categorical_accuracy: 1.0000 - val_loss: 1.0561e-06 - val_mean_absolute_error: 9.4157e-07 - val_categorical_accuracy: 1.0000
Epoch 5/5

  64/2908 [..............................] - ETA: 2:18 - loss: 1.6836e-05 - mean_absolute_error: 1.6715e-05 - categorical_accuracy: 1.0000
 128/2908 [>.............................] - ETA: 2:17 - loss: 8.4774e-06 - mean_absolute_error: 8.3573e-06 - categorical_accuracy: 1.0000
 192/2908 [>.............................] - ETA: 2:14 - loss: 5.7643e-06 - mean_absolute_error: 5.6460e-06 - categorical_accuracy: 1.0000
 256/2908 [=>............................] - ETA: 2:12 - loss: 4.3530e-06 - mean_absolute_error: 4.2345e-06 - categorical_accuracy: 1.0000
 320/2908 [==>...........................] - ETA: 2:09 - loss: 4.9995e-06 - mean_absolute_error: 4.8820e-06 - categorical_accuracy: 1.0000
 384/2908 [==>...........................] - ETA: 2:05 - loss: 4.1865e-06 - mean_absolute_error: 4.0689e-06 - categorical_accuracy: 1.0000
 448/2908 [===>..........................] - ETA: 2:02 - loss: 3.6084e-06 - mean_absolute_error: 3.4908e-06 - categorical_accuracy: 1.0000
 512/2908 [====>.........................] - ETA: 1:59 - loss: 3.1722e-06 - mean_absolute_error: 3.0545e-06 - categorical_accuracy: 1.0000
 576/2908 [====>.........................] - ETA: 1:56 - loss: 2.8330e-06 - mean_absolute_error: 2.7151e-06 - categorical_accuracy: 1.0000
 640/2908 [=====>........................] - ETA: 1:53 - loss: 2.6614e-06 - mean_absolute_error: 2.5443e-06 - categorical_accuracy: 1.0000
 704/2908 [======>.......................] - ETA: 1:49 - loss: 2.4303e-06 - mean_absolute_error: 2.3131e-06 - categorical_accuracy: 1.0000
 768/2908 [======>.......................] - ETA: 1:46 - loss: 2.2386e-06 - mean_absolute_error: 2.1216e-06 - categorical_accuracy: 1.0000
 832/2908 [=======>......................] - ETA: 1:43 - loss: 2.0783e-06 - mean_absolute_error: 1.9613e-06 - categorical_accuracy: 1.0000
 896/2908 [========>.....................] - ETA: 1:39 - loss: 1.9400e-06 - mean_absolute_error: 1.8231e-06 - categorical_accuracy: 1.0000
 960/2908 [========>.....................] - ETA: 1:36 - loss: 1.8189e-06 - mean_absolute_error: 1.7020e-06 - categorical_accuracy: 1.0000
1024/2908 [=========>....................] - ETA: 1:33 - loss: 1.1268e-05 - mean_absolute_error: 1.1105e-05 - categorical_accuracy: 1.0000
1088/2908 [==========>...................] - ETA: 1:30 - loss: 1.0612e-05 - mean_absolute_error: 1.0452e-05 - categorical_accuracy: 1.0000
1152/2908 [==========>...................] - ETA: 1:27 - loss: 1.0036e-05 - mean_absolute_error: 9.8776e-06 - categorical_accuracy: 1.0000
1216/2908 [===========>..................] - ETA: 1:23 - loss: 9.5146e-06 - mean_absolute_error: 9.3587e-06 - categorical_accuracy: 1.0000
1280/2908 [============>.................] - ETA: 1:20 - loss: 9.5722e-06 - mean_absolute_error: 9.4182e-06 - categorical_accuracy: 1.0000
1344/2908 [============>.................] - ETA: 1:17 - loss: 9.1358e-06 - mean_absolute_error: 8.9837e-06 - categorical_accuracy: 1.0000
1408/2908 [=============>................] - ETA: 1:14 - loss: 8.7263e-06 - mean_absolute_error: 8.5760e-06 - categorical_accuracy: 1.0000
1472/2908 [==============>...............] - ETA: 1:11 - loss: 8.3521e-06 - mean_absolute_error: 8.2031e-06 - categorical_accuracy: 1.0000
1536/2908 [==============>...............] - ETA: 1:07 - loss: 8.0103e-06 - mean_absolute_error: 7.8627e-06 - categorical_accuracy: 1.0000
1600/2908 [===============>..............] - ETA: 1:04 - loss: 7.6946e-06 - mean_absolute_error: 7.5482e-06 - categorical_accuracy: 1.0000
1664/2908 [================>.............] - ETA: 1:01 - loss: 7.5274e-06 - mean_absolute_error: 7.3823e-06 - categorical_accuracy: 1.0000
1728/2908 [================>.............] - ETA: 58s - loss: 7.2531e-06 - mean_absolute_error: 7.1091e-06 - categorical_accuracy: 1.0000 
1792/2908 [=================>............] - ETA: 54s - loss: 6.9983e-06 - mean_absolute_error: 6.8552e-06 - categorical_accuracy: 1.0000
1856/2908 [==================>...........] - ETA: 51s - loss: 7.5055e-06 - mean_absolute_error: 7.3630e-06 - categorical_accuracy: 1.0000
1920/2908 [==================>...........] - ETA: 48s - loss: 7.7040e-06 - mean_absolute_error: 7.5621e-06 - categorical_accuracy: 1.0000
1984/2908 [===================>..........] - ETA: 45s - loss: 7.4594e-06 - mean_absolute_error: 7.3182e-06 - categorical_accuracy: 1.0000
2048/2908 [====================>.........] - ETA: 42s - loss: 7.4688e-06 - mean_absolute_error: 7.3286e-06 - categorical_accuracy: 1.0000
2112/2908 [====================>.........] - ETA: 39s - loss: 7.2521e-06 - mean_absolute_error: 7.1128e-06 - categorical_accuracy: 1.0000
2176/2908 [=====================>........] - ETA: 36s - loss: 8.1434e-06 - mean_absolute_error: 8.0036e-06 - categorical_accuracy: 1.0000
2240/2908 [======================>.......] - ETA: 32s - loss: 7.9142e-06 - mean_absolute_error: 7.7749e-06 - categorical_accuracy: 1.0000
2304/2908 [======================>.......] - ETA: 29s - loss: 7.7041e-06 - mean_absolute_error: 7.5655e-06 - categorical_accuracy: 1.0000
2368/2908 [=======================>......] - ETA: 26s - loss: 7.4996e-06 - mean_absolute_error: 7.3617e-06 - categorical_accuracy: 1.0000
2432/2908 [========================>.....] - ETA: 23s - loss: 7.3215e-06 - mean_absolute_error: 7.1842e-06 - categorical_accuracy: 1.0000
2496/2908 [========================>.....] - ETA: 20s - loss: 7.1368e-06 - mean_absolute_error: 7.0000e-06 - categorical_accuracy: 1.0000
2560/2908 [=========================>....] - ETA: 17s - loss: 6.9623e-06 - mean_absolute_error: 6.8259e-06 - categorical_accuracy: 1.0000
2624/2908 [==========================>...] - ETA: 13s - loss: 6.7954e-06 - mean_absolute_error: 6.6594e-06 - categorical_accuracy: 1.0000
2688/2908 [==========================>...] - ETA: 10s - loss: 6.6367e-06 - mean_absolute_error: 6.5013e-06 - categorical_accuracy: 1.0000
2752/2908 [===========================>..] - ETA: 7s - loss: 6.4852e-06 - mean_absolute_error: 6.3501e-06 - categorical_accuracy: 1.0000 
2816/2908 [============================>.] - ETA: 4s - loss: 6.3522e-06 - mean_absolute_error: 6.2176e-06 - categorical_accuracy: 1.0000
2880/2908 [============================>.] - ETA: 1s - loss: 6.2137e-06 - mean_absolute_error: 6.0794e-06 - categorical_accuracy: 1.0000
2908/2908 [==============================] - 151s 52ms/step - loss: 6.1550e-06 - mean_absolute_error: 6.0209e-06 - categorical_accuracy: 1.0000 - val_loss: 9.5109e-07 - val_mean_absolute_error: 8.3634e-07 - val_categorical_accuracy: 1.0000
Stop learning 2019-01-05 22:28:18.734894
Elapsed learning time 0:12:41.671843
True
[[1.0000000e+00 6.6709672e-25]
 [6.1934367e-15 1.0000000e+00]
 [1.6817134e-12 1.0000000e+00]
 ...
 [1.0000000e+00 2.0982647e-17]
 [8.5347776e-16 1.0000000e+00]
 [5.2003208e-14 1.0000000e+00]]
[[1. 0.]
 [0. 1.]
 [0. 1.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]
