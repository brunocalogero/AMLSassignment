full dataset of shape: (4546, 128, 128, 3)
full labels of shape: (4546,)
X_data of shape: (4546, 128, 128, 3)
Y_data of shape: (4546,)
X_train of shape: (2908, 128, 128, 3)
y_train of shape: (2908, 2)
X_val of shape: (728, 128, 128, 3)
y_val of shape: (728, 2)
X_test of shape: (910, 128, 128, 3)
y_test of shape: (910, 2)
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
[187.0849381  190.80914718 194.27063274 188.85350757 192.59731774
 196.07255846 189.66093535 193.36176066 196.85591472 190.05570839]
X_train of shape: (2908, 49152)
X_val of shape: (728, 49152)
X_test of shape: (910, 49152)
X_train of shape: (2908, 128, 128, 3)
X_val of shape: (728, 128, 128, 3)
X_test of shape: (910, 128, 128, 3)
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 128, 128, 64)      1792
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 128, 64)      256
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 64, 64, 64)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 64, 32)        18464
_________________________________________________________________
batch_normalization_2 (Batch (None, 64, 64, 32)        128
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32, 32)        0
_________________________________________________________________
flatten_1 (Flatten)          (None, 32768)             0
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 65538
=================================================================
Total params: 86,178
Trainable params: 85,986
Non-trainable params: 192
_________________________________________________________________
Start learning with best params at 2019-01-05 18:38:56.917623
Train on 2908 samples, validate on 728 samples
Epoch 1/15

  64/2908 [..............................] - ETA: 4:45 - loss: 1.5466 - acc: 0.4844
 128/2908 [>.............................] - ETA: 4:44 - loss: 0.7733 - acc: 0.7422
 192/2908 [>.............................] - ETA: 4:33 - loss: 0.5155 - acc: 0.8281
 256/2908 [=>............................] - ETA: 4:22 - loss: 0.3867 - acc: 0.8711
 320/2908 [==>...........................] - ETA: 4:13 - loss: 0.3093 - acc: 0.8969
 384/2908 [==>...........................] - ETA: 4:05 - loss: 0.2578 - acc: 0.9141
 448/2908 [===>..........................] - ETA: 4:03 - loss: 0.2209 - acc: 0.9263
 512/2908 [====>.........................] - ETA: 3:56 - loss: 0.1933 - acc: 0.9355
 576/2908 [====>.........................] - ETA: 3:48 - loss: 0.1718 - acc: 0.9427
 640/2908 [=====>........................] - ETA: 3:41 - loss: 0.1547 - acc: 0.9484
 704/2908 [======>.......................] - ETA: 3:35 - loss: 0.1406 - acc: 0.9531
 768/2908 [======>.......................] - ETA: 3:29 - loss: 0.1289 - acc: 0.9570
 832/2908 [=======>......................] - ETA: 3:24 - loss: 0.1190 - acc: 0.9603
 896/2908 [========>.....................] - ETA: 3:18 - loss: 0.1105 - acc: 0.9632
 960/2908 [========>.....................] - ETA: 3:11 - loss: 0.1031 - acc: 0.9656
1024/2908 [=========>....................] - ETA: 3:04 - loss: 0.0967 - acc: 0.9678
1088/2908 [==========>...................] - ETA: 2:58 - loss: 0.0910 - acc: 0.9697
1152/2908 [==========>...................] - ETA: 2:51 - loss: 0.0859 - acc: 0.9714
1216/2908 [===========>..................] - ETA: 2:44 - loss: 0.0814 - acc: 0.9729
1280/2908 [============>.................] - ETA: 2:38 - loss: 0.0773 - acc: 0.9742
1344/2908 [============>.................] - ETA: 2:32 - loss: 0.0737 - acc: 0.9754
1408/2908 [=============>................] - ETA: 2:25 - loss: 0.0703 - acc: 0.9766
1472/2908 [==============>...............] - ETA: 2:19 - loss: 0.0672 - acc: 0.9776
1536/2908 [==============>...............] - ETA: 2:13 - loss: 0.0644 - acc: 0.9785
1600/2908 [===============>..............] - ETA: 2:07 - loss: 0.0619 - acc: 0.9794
1664/2908 [================>.............] - ETA: 2:00 - loss: 0.0595 - acc: 0.9802
1728/2908 [================>.............] - ETA: 1:54 - loss: 0.0573 - acc: 0.9809
1792/2908 [=================>............] - ETA: 1:48 - loss: 0.0552 - acc: 0.9816
1856/2908 [==================>...........] - ETA: 1:42 - loss: 0.0533 - acc: 0.9822
1920/2908 [==================>...........] - ETA: 1:36 - loss: 0.0516 - acc: 0.9828
1984/2908 [===================>..........] - ETA: 1:30 - loss: 0.0499 - acc: 0.9834
2048/2908 [====================>.........] - ETA: 1:23 - loss: 0.0483 - acc: 0.9839
2112/2908 [====================>.........] - ETA: 1:17 - loss: 0.0469 - acc: 0.9844
2176/2908 [=====================>........] - ETA: 1:11 - loss: 0.0455 - acc: 0.9848
2240/2908 [======================>.......] - ETA: 1:05 - loss: 0.0442 - acc: 0.9853
2304/2908 [======================>.......] - ETA: 58s - loss: 0.0430 - acc: 0.9857
2368/2908 [=======================>......] - ETA: 52s - loss: 0.0418 - acc: 0.9861
2432/2908 [========================>.....] - ETA: 46s - loss: 0.0407 - acc: 0.9864
2496/2908 [========================>.....] - ETA: 40s - loss: 0.0397 - acc: 0.9868
2560/2908 [=========================>....] - ETA: 33s - loss: 0.0387 - acc: 0.9871
2624/2908 [==========================>...] - ETA: 27s - loss: 0.0377 - acc: 0.9874
2688/2908 [==========================>...] - ETA: 21s - loss: 0.0368 - acc: 0.9877
2752/2908 [===========================>..] - ETA: 15s - loss: 0.0360 - acc: 0.9880
2816/2908 [============================>.] - ETA: 8s - loss: 0.0352 - acc: 0.9883
2880/2908 [============================>.] - ETA: 2s - loss: 0.0344 - acc: 0.9885
2908/2908 [==============================] - 299s 103ms/step - loss: 0.0340 - acc: 0.9887 - val_loss: 0.0971 - val_acc: 0.9863
Epoch 2/15

  64/2908 [..............................] - ETA: 5:07 - loss: 1.1921e-07 - acc: 1.0000
 128/2908 [>.............................] - ETA: 4:37 - loss: 1.1921e-07 - acc: 1.0000
 192/2908 [>.............................] - ETA: 4:25 - loss: 1.1921e-07 - acc: 1.0000
 256/2908 [=>............................] - ETA: 4:17 - loss: 1.1921e-07 - acc: 1.0000
 320/2908 [==>...........................] - ETA: 4:13 - loss: 1.1921e-07 - acc: 1.0000
 384/2908 [==>...........................] - ETA: 4:06 - loss: 1.1921e-07 - acc: 1.0000
 448/2908 [===>..........................] - ETA: 3:59 - loss: 1.1921e-07 - acc: 1.0000
 512/2908 [====>.........................] - ETA: 3:54 - loss: 1.1921e-07 - acc: 1.0000
 576/2908 [====>.........................] - ETA: 3:48 - loss: 1.1921e-07 - acc: 1.0000
 640/2908 [=====>........................] - ETA: 3:42 - loss: 1.1921e-07 - acc: 1.0000
 704/2908 [======>.......................] - ETA: 3:36 - loss: 1.1921e-07 - acc: 1.0000
 768/2908 [======>.......................] - ETA: 3:30 - loss: 1.1921e-07 - acc: 1.0000
 832/2908 [=======>......................] - ETA: 3:25 - loss: 1.1921e-07 - acc: 1.0000
 896/2908 [========>.....................] - ETA: 3:19 - loss: 1.1921e-07 - acc: 1.0000
 960/2908 [========>.....................] - ETA: 3:13 - loss: 1.1921e-07 - acc: 1.0000
1024/2908 [=========>....................] - ETA: 3:06 - loss: 1.1921e-07 - acc: 1.0000
1088/2908 [==========>...................] - ETA: 3:00 - loss: 1.1921e-07 - acc: 1.0000
1152/2908 [==========>...................] - ETA: 2:53 - loss: 1.1921e-07 - acc: 1.0000
1216/2908 [===========>..................] - ETA: 2:47 - loss: 1.1921e-07 - acc: 1.0000
1280/2908 [============>.................] - ETA: 2:40 - loss: 1.1921e-07 - acc: 1.0000
1344/2908 [============>.................] - ETA: 2:34 - loss: 1.1921e-07 - acc: 1.0000
1408/2908 [=============>................] - ETA: 2:29 - loss: 1.1921e-07 - acc: 1.0000
1472/2908 [==============>...............] - ETA: 2:22 - loss: 1.1921e-07 - acc: 1.0000
1536/2908 [==============>...............] - ETA: 2:16 - loss: 1.1921e-07 - acc: 1.0000
1600/2908 [===============>..............] - ETA: 2:10 - loss: 1.1921e-07 - acc: 1.0000
1664/2908 [================>.............] - ETA: 2:04 - loss: 1.1921e-07 - acc: 1.0000
1728/2908 [================>.............] - ETA: 1:57 - loss: 1.1921e-07 - acc: 1.0000
1792/2908 [=================>............] - ETA: 1:51 - loss: 1.1921e-07 - acc: 1.0000
1856/2908 [==================>...........] - ETA: 1:45 - loss: 1.1921e-07 - acc: 1.0000
1920/2908 [==================>...........] - ETA: 1:38 - loss: 1.1921e-07 - acc: 1.0000
1984/2908 [===================>..........] - ETA: 1:32 - loss: 1.1921e-07 - acc: 1.0000
2048/2908 [====================>.........] - ETA: 1:26 - loss: 1.1921e-07 - acc: 1.0000
2112/2908 [====================>.........] - ETA: 1:19 - loss: 1.1921e-07 - acc: 1.0000
2176/2908 [=====================>........] - ETA: 1:13 - loss: 1.1921e-07 - acc: 1.0000
2240/2908 [======================>.......] - ETA: 1:07 - loss: 1.1921e-07 - acc: 1.0000
2304/2908 [======================>.......] - ETA: 1:00 - loss: 1.1921e-07 - acc: 1.0000
2368/2908 [=======================>......] - ETA: 54s - loss: 1.1921e-07 - acc: 1.0000
2432/2908 [========================>.....] - ETA: 47s - loss: 1.1921e-07 - acc: 1.0000
2496/2908 [========================>.....] - ETA: 41s - loss: 1.1921e-07 - acc: 1.0000
2560/2908 [=========================>....] - ETA: 34s - loss: 1.1921e-07 - acc: 1.0000
2624/2908 [==========================>...] - ETA: 28s - loss: 1.1921e-07 - acc: 1.0000
2688/2908 [==========================>...] - ETA: 22s - loss: 1.1921e-07 - acc: 1.0000
2752/2908 [===========================>..] - ETA: 15s - loss: 1.1921e-07 - acc: 1.0000
2816/2908 [============================>.] - ETA: 9s - loss: 1.1921e-07 - acc: 1.0000
2880/2908 [============================>.] - ETA: 2s - loss: 1.1921e-07 - acc: 1.0000

epochs = 15
batch_size = 64

# Function to create model, required for KerasClassifier
def create_model(optimizer='adam', learn_rate=0.001, amsgrad=False, activation=tf.nn.leaky_relu):

    channel_1, channel_2, channel_3, num_classes =  64, 32, 8, 2
    # create model
    model = Sequential()

    model.add(Conv2D(channel_1, (3, 3), padding='SAME', activation=activation,  input_shape=(128, 128, 3), data_format="channels_last"))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))

    model.add(Dropout(0.2))

    model.add(Conv2D(channel_2, (3, 3), padding='SAME', activation=activation))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))

    # model.add(Conv2D(channel_3, (3, 3), padding='SAME', activation=activation))
    # model.add(BatchNormalization())
    # model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))
    model.add(Dropout(0.5))

    model.add(Flatten())
    model.add(Dense(num_classes, activation='softmax'))

    optimizer = Adam(lr=learn_rate, amsgrad=amsgrad )
    # Compile model (sparse cross-entropy can be used if one hot encoding not used)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])

    return model
